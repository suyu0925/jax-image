{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd166241",
   "metadata": {},
   "source": [
    "In this first section you will learn the very fundamentals of JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4cd439",
   "metadata": {},
   "source": [
    "## Getting started with JAX numpy\n",
    "Fundamentally, JAX is a library that enables transformations of array-manipulating programs written with a NumPy-like API.\n",
    "\n",
    "Over the course of this series of guides, we will unpack exactly what that means. For now, you can think of JAX as differentiable NumPy that runs on accelerators.\n",
    "\n",
    "The code below shows how to import JAX and create a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca5eceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.arange(10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165a895",
   "metadata": {},
   "source": [
    "So far, everything is just like NumPy. A big appeal of JAX is that you don’t need to learn a new API. Many common NumPy programs would run just as well in JAX if you substitute np for jnp. However, there are some important differences which we touch on at the end of this section.\n",
    "\n",
    "You can notice the first difference if you check the type of x. It is a variable of type DeviceArray, which is the way JAX represents arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e71f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6830b9d",
   "metadata": {},
   "source": [
    "One useful feature of JAX is that the same code can be run on different backends – CPU, GPU and TPU.\n",
    "\n",
    "We will now perform a dot product to demonstrate that it can be done in different devices without changing the code. We use %timeit to check the performance.\n",
    "\n",
    "(Technical detail: when a JAX function is called, the corresponding operation is dispatched to an accelerator to be computed asynchronously when possible. The returned array is therefore not necessarily ‘filled in’ as soon as the function returns. Thus, if we don’t require the result immediately, the computation won’t block Python execution. Therefore, unless we block_until_ready, we will only time the dispatch, not the actual computation. See Asynchronous dispatch in the JAX docs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8751f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 µs ± 26.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "long_vector = jnp.arange(int(1e7))\n",
    "\n",
    "%timeit jnp.dot(long_vector, long_vector).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e9461",
   "metadata": {},
   "source": [
    "**Tip**: Try running the code above twice, once without an accelerator, and once with a GPU runtime (while in Colab, click Runtime → Change Runtime Type and choose GPU). Notice how much faster it runs on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da58eaf",
   "metadata": {},
   "source": [
    "## JAX first transformation: grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7894f",
   "metadata": {},
   "source": [
    "A fundamental feature of JAX is that it allows you to transform functions.\n",
    "\n",
    "One of the most commonly used transformations is `jax.grad`, which takes a numerical function written in Python and returns you a new Python function that computes the gradient of the original function.\n",
    "\n",
    "To use it, let’s first define a function that takes an array and returns the sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560f46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(x):\n",
    "  return jnp.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd56989",
   "metadata": {},
   "source": [
    "Applying `jax.grad` to `sum_of_squares` will return a different function, namely the gradient of `sum_of_squares` with respect to its first parameter `x`.\n",
    "\n",
    "Then, you can use that function on an array to return the derivatives with respect to each element of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db68633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "[2. 4. 6. 8.]\n"
     ]
    }
   ],
   "source": [
    "sum_of_squares_dx = jax.grad(sum_of_squares)\n",
    "\n",
    "x = jnp.asarray([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "print(sum_of_squares(x))\n",
    "\n",
    "print(sum_of_squares_dx(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
